{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "from tqdm import tqdm\n",
    "import struct\n",
    "import pickle\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "import scipy as sp\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "np.seterr('raise')\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "def read_complex_binary2(filename):\n",
    "    \"\"\" Read file of float32 into complex array.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        bytes = f.read()\n",
    "    data = np.frombuffer(bytes, dtype=np.float32).reshape(-1, 2)\n",
    "    data = data[:, 0] + 1j*data[:, 1]\n",
    "    return data\n",
    "\n",
    "def get_rssis(filename):\n",
    "    \"\"\" Get RSSI time series from file.\n",
    "    \"\"\"\n",
    "    rssis = []\n",
    "    data = read_complex_binary2(filename)\n",
    "    num_frame = int(len(data)/65536 - 2)\n",
    "    for i in range(num_frame):\n",
    "        section = data[i*65536:(i+1)*65536]\n",
    "        spectrum = 10*np.log10(np.fft.fftshift(np.abs(np.fft.fft(section))**2))\n",
    "        rssis.append(np.max(spectrum[22929:22949]))\n",
    "    rssis = np.array(rssis)\n",
    "    return rssis\n",
    "\n",
    "def get_test_grid(dirname, prefix, xs=None, ys=None):\n",
    "    \"\"\" Get test grid from xs, ys.\n",
    "    \"\"\"\n",
    "    test_rssis = {}\n",
    "    \n",
    "    if xs is None or ys is None:\n",
    "        for filename in tqdm(glob.glob(os.path.join(dirname, f\"{prefix}*.dat\"))):\n",
    "            coords = os.path.splitext(os.path.basename(filename))[0][1:]\n",
    "            x, y = int(coords[:2]), int (coords[2:])\n",
    "            \n",
    "            filename_cached = os.path.splitext(filename)[0] + '.pkl'\n",
    "            if os.path.exists(filename_cached):\n",
    "                with open(filename_cached, 'rb') as f:\n",
    "                    mean, std = pickle.load(f)\n",
    "                test_rssis[x,y] = mean\n",
    "            else:\n",
    "                rssis = get_rssis(filename)\n",
    "                if len(rssis):\n",
    "                    mean, std = np.mean(rssis), np.std(rssis)\n",
    "                    with open(filename_cached, 'wb') as f:\n",
    "                        pickle.dump((mean,std), f)\n",
    "                    test_rssis[x,y] = mean\n",
    "                \n",
    "    else:\n",
    "        for i, x in enumerate(tqdm(xs)):\n",
    "            for j, y in enumerate(ys):\n",
    "                filename = os.path.join(dirname, f\"{prefix}{x:>02}{y:>02}.dat\")\n",
    "                filename_cached = os.path.splitext(filename)[0] + '.pkl'\n",
    "                if os.path.exists(filename_cached):\n",
    "                    with open(filename_cached, 'rb') as f:\n",
    "                        mean, std = pickle.load(f)\n",
    "                    test_rssis[x,y] = mean\n",
    "                else:\n",
    "                    rssis = get_rssis(filename)\n",
    "                    if len(rssis):\n",
    "                        mean, std = np.mean(rssis), np.std(rssis)\n",
    "                        with open(filename_cached, 'wb') as f:\n",
    "                            pickle.dump((mean,std), f)\n",
    "                        test_rssis[x,y] = mean\n",
    "    \n",
    "    return test_rssis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_model_mle(dirname, prefix, xs, ys):\n",
    "    \"\"\" Get the likelihood model from directory of data files.\n",
    "    \"\"\"\n",
    "    s_mean = np.full_like(np.meshgrid(xs,ys)[0], np.nan, dtype=np.float64)\n",
    "    s_dev  = np.full_like(np.meshgrid(xs,ys)[0], np.nan, dtype=np.float64)\n",
    "\n",
    "    for i, x in enumerate(tqdm(xs)):\n",
    "        for j, y in enumerate(ys):\n",
    "            filename = os.path.join(dirname, f\"{prefix}{x:>02}{y:>02}.dat\")\n",
    "            filename_cached = os.path.splitext(filename)[0] + '.pkl'\n",
    "            if os.path.exists(filename_cached):\n",
    "                with open(filename_cached, 'rb') as f:\n",
    "                    mean, std = pickle.load(f)\n",
    "                s_mean[i,j] = mean\n",
    "                s_dev[i,j] = std\n",
    "            else:\n",
    "                rssis = get_rssis(filename)\n",
    "                if len(rssis):\n",
    "                    mean, std = np.mean(rssis), np.std(rssis)\n",
    "                    with open(filename_cached, 'wb') as f:\n",
    "                        pickle.dump((mean,std), f)\n",
    "                    s_mean[i,j] = mean\n",
    "                    s_dev[i,j] = std\n",
    "            \n",
    "    f_mean = RectBivariateSpline(xs, ys, s_mean)\n",
    "    f_dev = RectBivariateSpline(xs, ys, s_dev)\n",
    "    \n",
    "    return f_mean, f_dev \n",
    "\n",
    "def log_normal_pdf(x, mean, stddev):\n",
    "    \"\"\" Compute log of normal pdf.\n",
    "    \"\"\"\n",
    "    result = -0.5*np.log(2*np.pi*stddev**2) - (x-mean)**2/(2*stddev**2)\n",
    "    return result\n",
    "\n",
    "def calc_log_likelihood(rssis, x, y, f_mean, f_dev, c=0, n=0):\n",
    "    \"\"\" Compute likelihood of observing 4 rssis measurements given the ground truth\n",
    "        location is x, y for a model given by f_mean, f_dev.\n",
    "    \"\"\"\n",
    "\n",
    "    log_likelihood = 0.0\n",
    "    log_likelihood += log_normal_pdf(rssis[0], f_mean(x, y) + c, f_dev(x, y))\n",
    "    log_likelihood += log_normal_pdf(rssis[1], f_mean(n-y, x) + c, f_dev(n-y, x))\n",
    "    log_likelihood += log_normal_pdf(rssis[2], f_mean(n-x, n-y) + c, f_dev(n-x, n-y))\n",
    "    log_likelihood += log_normal_pdf(rssis[3], f_mean(y, n-x) + c, f_dev(y, n-x))\n",
    "    return log_likelihood\n",
    "\n",
    "def localize_mle(rssis, f_mean, f_dev, ls, n):\n",
    "    \"\"\" Compute location estimate using maximum-likelihood estimation.\n",
    "    \"\"\"\n",
    "    xs = np.arange(0, n+0.1, 0.1)\n",
    "    ys = np.arange(0, n+0.1, 0.1)\n",
    "    \n",
    "    likelihood_grid = np.zeros_like(np.meshgrid(xs,ys,ls)[0], dtype=np.float64)\n",
    "    for i, x in enumerate(xs):\n",
    "        for j, y in enumerate(ys):\n",
    "            for k, l in enumerate(ls):\n",
    "                likelihood_grid[i,j,k] = calc_log_likelihood(rssis, x, y, f_mean, f_dev, l, n)\n",
    "    result = np.unravel_index(np.argmax(likelihood_grid), likelihood_grid.shape)\n",
    "    \n",
    "    return result[0]/10.0, result[1]/10.0, result[2]\n",
    "\n",
    "def run_mle(dirname, n,\n",
    "            train_prefix, test_prefix, \n",
    "            train_xs, train_ys, test_xs, test_ys):\n",
    "    \n",
    "    test_grid     = get_test_grid(dirname, test_prefix, test_xs, test_ys)\n",
    "    f_mean, f_dev = get_model_mle(dirname, train_prefix, train_xs, train_ys)\n",
    "\n",
    "    errors = []\n",
    "    prediction_record=[]\n",
    "    for (x,y) in tqdm(test_grid.keys()):\n",
    "        rssis = [test_grid[x, y], \n",
    "                 test_grid[n-y, x],\n",
    "                 test_grid[n-x, n-y], \n",
    "                 test_grid[y, n-x]]\n",
    "        x_pred, y_pred, l = localize_mle(rssis, f_mean, f_dev, np.arange(-3, 10), n)\n",
    "        errors.append(np.sqrt((x-x_pred)**2+(y-y_pred)**2))\n",
    "        prediction_record.append([x,y,x_pred,y_pred,l])\n",
    "    print(f\"mean error: {np.mean(errors)}\")\n",
    "    print(f\"median error: {np.median(errors)}\")\n",
    "    print(f\"stdev error: {np.std(errors)}\")\n",
    "    \n",
    "    return errors, prediction_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE w/o $\\Lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mle_nl(dirname, n,\n",
    "               train_prefix, test_prefix, \n",
    "               train_xs, train_ys, test_xs, test_ys):\n",
    "    \n",
    "    test_grid     = get_test_grid(dirname, test_prefix, test_xs, test_ys)\n",
    "    f_mean, f_dev = get_model_mle(dirname, train_prefix, train_xs, train_ys)\n",
    "    \n",
    "    errors = []\n",
    "    prediction_record=[]\n",
    "    for (x,y) in tqdm(test_grid.keys()):\n",
    "        rssis = [test_grid[x, y], \n",
    "                 test_grid[n-y, x],\n",
    "                 test_grid[n-x, n-y], \n",
    "                 test_grid[y, n-x]]\n",
    "        x_pred, y_pred, l = localize_mle(rssis, f_mean, f_dev, np.arange(0, 1), n)\n",
    "        errors.append(np.sqrt((x-x_pred)**2+(y-y_pred)**2))\n",
    "        prediction_record.append([x,y,x_pred,y_pred,l])\n",
    "    print(f\"mean error: {np.mean(errors)}\")\n",
    "    print(f\"median error: {np.median(errors)}\")\n",
    "    print(f\"stdev error: {np.std(errors)}\")\n",
    "    \n",
    "    return errors, prediction_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 73/73 [00:00<00:00, 45536.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 8683.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:25<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error: 0.6328864463788993\n",
      "median error: 0.5385164807134504\n",
      "stdev error: 0.21987186013398202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 118/118 [00:00<00:00, 47511.56it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 8197.34it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 61/61 [00:44<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error: 1.429874502841358\n",
      "median error: 1.118033988749895\n",
      "stdev error: 1.109740726428778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52/52 [00:00<00:00, 43326.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 8817.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:17<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error: 1.2077446402503176\n",
      "median error: 0.8246211251235319\n",
      "stdev error: 0.7635135126156125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "errors_mle_nl = []\n",
    "\n",
    "# Dataset\n",
    "# dirname = 'data/Sep 17- redo water'\n",
    "# n = 10 # grid is 0 to n (n+1 by n+1)\n",
    "# # AEM*\n",
    "# train_prefix = 0\n",
    "# train_xs = np.arange(0,n+1,2)\n",
    "# train_ys = np.arange(0,n+1,2)\n",
    "# test_prefix = 1\n",
    "# # Run prediction\n",
    "# errors, prediction_record = run_mle_nl(dirname, n,\n",
    "#                                        train_prefix, test_prefix,\n",
    "#                                        train_xs, train_ys, None, None)\n",
    "# errors_mle_nl += errors\n",
    "\n",
    "# Dataset\n",
    "dirname = 'data/sep-18-first'\n",
    "n = 10 # grid is 0 to n (n+1 by n+1)\n",
    "# AEM*\n",
    "train_prefix = 0\n",
    "train_xs = np.arange(0,n+1,2)\n",
    "train_ys = np.arange(0,n+1,2)\n",
    "test_prefix = 1\n",
    "# Run prediction\n",
    "errors, prediction_record = run_mle_nl(dirname, n,\n",
    "                                       train_prefix, test_prefix,\n",
    "                                       train_xs, train_ys, None, None)\n",
    "errors_mle_nl += errors\n",
    "\n",
    "# Dataset\n",
    "dirname = 'data/sep-19-full'\n",
    "n = 10 # grid is 0 to n (n+1 by n+1)\n",
    "# AEM*\n",
    "train_prefix = 0\n",
    "train_xs = np.arange(0,n+1,2)\n",
    "train_ys = np.arange(0,n+1,2)\n",
    "test_prefix = 1\n",
    "# Run prediction\n",
    "errors, prediction_record = run_mle_nl(dirname, n,\n",
    "                                       train_prefix, test_prefix,\n",
    "                                       train_xs, train_ys, None, None)\n",
    "errors_mle_nl += errors\n",
    "\n",
    "# Dataset\n",
    "dirname = 'data/sep-20-redo'\n",
    "n = 10 # grid is 0 to n (n+1 by n+1)\n",
    "# AEM*\n",
    "train_prefix = 0\n",
    "train_xs = np.arange(0,n+1,2)\n",
    "train_ys = np.arange(0,n+1,2)\n",
    "test_prefix = 1\n",
    "# Run prediction\n",
    "errors, prediction_record = run_mle_nl(dirname, n,\n",
    "                                       train_prefix, test_prefix,\n",
    "                                       train_xs, train_ys, None, None)\n",
    "errors_mle_nl += errors\n",
    "\n",
    "with open('errors_mle_nl.pkl', 'wb') as f:\n",
    "    pickle.dump(errors_mle_nl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
